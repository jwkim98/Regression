{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data\n",
    "# Split the data into a training set of first 8000 instances, validation set of next 1000 instances\n",
    "# and a test set of the rest 1000 instances\n",
    "# uniformly distributed random noise (-3, 3) is used to generate x1_array and x2_array\n",
    "x1_array = np.random.uniform(-3, 3, num_data)\n",
    "x2_array = np.random.uniform(-3, 3, num_data)\n",
    "input_array = np.empty([num_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,num_data):\n",
    "    input = 2.0 + 1.0*x1_array[i] + 0.5*pow(x1_array[i],2) + 0.25*pow(x1_array[i],3) + 0.5*pow(x2_array[i],2) + np.random.normal(0,1)\n",
    "    input_array[i] = input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_y = input_array[:8000]\n",
    "validation_set_y = input_array[8000:9000]\n",
    "test_set_y = input_array[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x1 = x1_array[:8000]\n",
    "validation_set_x1 = x1_array[8000:9000]\n",
    "test_set_x1 = x1_array[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "train_set_x2 = x2_array[:8000]\n",
    "validation_set_x2 = x2_array[8000:9000]\n",
    "test_set_x2 = x2_array[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. We learn (3 + 5)!/(3!*5!) parameters here which is 56. Therefore, we learn 56 parameters\n",
    "poly = PolynomialFeatures(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. We want to regularize y = a + b*x1 + c*x1^2 + d*x1^3 + e*x2^2\n",
    "# Using polynomial regression, we should minimize (1/2)(X*theta - y)^T(X*theta - y)\n",
    "# Which gives us derivative X^T(X*theta - y)\n",
    "def polynomial(params, x1, x2):\n",
    "    val = params[0] + params[1]*x1 + params[2]*pow(x1,2) + params[3]*pow(x1,3) + params[4]*pow(x2,2)\n",
    "    #print(\"val: \" + str(val))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# partial derivative for all parameters\n",
    "def polynomial_derivative(params, x1, x2, index):\n",
    "    val = 0\n",
    "    if index == 0:\n",
    "        val =  1\n",
    "    elif index == 1:\n",
    "        val = x1\n",
    "    elif index == 2:\n",
    "        val = pow(x1,2)\n",
    "    elif index == 3:\n",
    "        val = pow(x1,3)\n",
    "    elif index == 4:\n",
    "        val = pow(x2,2)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rmse(params, test_set_x1, test_set_x2, test_set_y):\n",
    "    sum = 0\n",
    "    for i in range(0, len(test_set_x1)):\n",
    "        sum += pow(polynomial(params,test_set_x1[i], test_set_x2[i]) - test_set_y[i], 2)\n",
    "    # print(sum)\n",
    "    return math.sqrt(sum/len(test_set_x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(epochs, learning_rate):\n",
    "    num_params = 5\n",
    "    params = [0,0,0,0,0]\n",
    "    validation_error_list = []\n",
    "    train_error_list = []\n",
    "\n",
    "\n",
    "    for j in range(0, epochs):\n",
    "        for k in range(0, num_params):\n",
    "            update = (polynomial(params, train_set_x1[j], train_set_x2[j]) - train_set_y[j]) * polynomial_derivative(params, train_set_x1[j], train_set_x2[j], k)\n",
    "            params[k] = params[k] - learning_rate*update\n",
    "        validation_error = evaluate_rmse(params, validation_set_x1, validation_set_x2, validation_set_y)\n",
    "        train_error = evaluate_rmse(params, train_set_x1, train_set_x2, train_set_y)\n",
    "        validation_error_list.append(validation_error)\n",
    "        train_error_list.append(train_error)\n",
    "\n",
    "        # print(\"Epoch: \" + str(j) + \" validation_error : \" + str(validation_error) + \" train_error : \" + str(train_error))\n",
    "\n",
    "    test_rmse = evaluate_rmse(params, test_set_x1, test_set_x2, test_set_y)\n",
    "    print(\"Test RMSE gradient_descent: \" + str(test_rmse))\n",
    "\n",
    "    plt.plot(list(range(0, epochs)), train_error_list, c = 'b')\n",
    "    plt.plot(list(range(0, epochs)), validation_error_list, c = 'r')\n",
    "    plt.savefig(\"Gradient_descent.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ridge_regression(epochs, learning_rate, alpha):\n",
    "    num_params = 5\n",
    "    params = [0,0,0,0,0]\n",
    "    validation_error_list = []\n",
    "    train_error_list = []\n",
    "\n",
    "    for j in range(0, epochs):\n",
    "        for k in range(0, num_params):\n",
    "            # update = X^t(X*theta - y) + alpha * theta\n",
    "            theta = 0 if k == 0 else params[k]\n",
    "            update = (polynomial(params, train_set_x1[j], train_set_x2[j]) - train_set_y[j]) * polynomial_derivative(params, train_set_x1[j], train_set_x2[j], k) + alpha*theta\n",
    "            params[k] -= learning_rate*update\n",
    "\n",
    "        validation_error = evaluate_rmse(params, validation_set_x1, validation_set_x2, validation_set_y)\n",
    "        train_error = evaluate_rmse(params, train_set_x1, train_set_x2, train_set_y)\n",
    "        validation_error_list.append(validation_error)\n",
    "        train_error_list.append(train_error)\n",
    "\n",
    "        # print(\"Epoch: \" + str(i) + \"validation_error : \" + str(validation_error) + \"train_error : \" + str(train_error))\n",
    "\n",
    "    test_rmse = evaluate_rmse(params, test_set_x1, test_set_x2, test_set_y)\n",
    "    print(\"Test RMSE ridge_regression: \" + str(test_rmse))\n",
    "\n",
    "    plt.plot(list(range(0, epochs)), train_error_list, c = 'b')\n",
    "    plt.plot(list(range(0, epochs)), validation_error_list, c = 'r')\n",
    "    plt.savefig(\"ridge_regression.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression(epochs, learning_rate, alpha):\n",
    "    num_params = 5\n",
    "    params = [0,0,0,0,0]\n",
    "    validation_error_list = []\n",
    "    train_error_list = []\n",
    "\n",
    "    for j in range(0, epochs):\n",
    "        for k in range(0, num_params):\n",
    "\n",
    "            theta = 0\n",
    "            if k > 0:\n",
    "                if params[k] > 0:\n",
    "                    theta = 1\n",
    "                elif params[k] < 0:\n",
    "                    theta = -1\n",
    "                else:\n",
    "                    theta = 0\n",
    "\n",
    "            update = (polynomial(params, train_set_x1[j], train_set_x2[j]) - train_set_y[j]) * polynomial_derivative(params, train_set_x1[j], train_set_x2[j], k) + alpha*theta\n",
    "            params[k] -= learning_rate*update\n",
    "\n",
    "        validation_error = evaluate_rmse(params, validation_set_x1, validation_set_x2, validation_set_y)\n",
    "        train_error = evaluate_rmse(params, train_set_x1, train_set_x2, train_set_y)\n",
    "        validation_error_list.append(validation_error)\n",
    "        train_error_list.append(train_error)\n",
    "\n",
    "        # print(\"Epoch: \" + str(j) + \" validation_error : \" + str(validation_error) + \"train_error : \" + str(train_error))\n",
    "\n",
    "    test_rmse = evaluate_rmse(params, test_set_x1, test_set_x2, test_set_y)\n",
    "    print(\"Test RMSE lasso_regression: \" + str(test_rmse))\n",
    "\n",
    "    plt.plot(list(range(0, epochs)), train_error_list, c = 'b')\n",
    "    plt.plot(list(range(0, epochs)), validation_error_list, c = 'r')\n",
    "    plt.savefig(\"lasso_regression.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def linear_regression_batch(epochs, learning_rate, batch_size):\n",
    "    num_params = 5\n",
    "    params = [0,0,0,0,0]\n",
    "    validation_error_list = []\n",
    "    train_error_list = []\n",
    "\n",
    "    tuple_list = []\n",
    "\n",
    "    for i in range(0, len(train_set_x1)):\n",
    "        tuple_list.append((train_set_x1[i], train_set_x2[i], train_set_y[i]))\n",
    "\n",
    "    random.shuffle(tuple_list)\n",
    "    train_data_list = tuple_list[:batch_size]\n",
    "\n",
    "    for i in range(0, epochs):\n",
    "        for j in range(0, len(train_data_list)):\n",
    "            for k in range(0, num_params):\n",
    "                update = (polynomial(params, train_data_list[j][0], train_data_list[j][1]) - train_data_list[j][2]) * polynomial_derivative(params,train_data_list[j][0], train_data_list[j][1], k)\n",
    "                params[k] -= learning_rate*update\n",
    "\n",
    "        validation_error = evaluate_rmse(params, validation_set_x1, validation_set_x2, validation_set_y)\n",
    "        train_error = evaluate_rmse(params, train_set_x1, train_set_x2, train_set_y)\n",
    "        validation_error_list.append(validation_error)\n",
    "        train_error_list.append(train_error)\n",
    "\n",
    "        # print(\"Epoch: \" + str(i) + \"validation_error : \" + str(validation_error) + \"train_error : \" + str(train_error))\n",
    "    \n",
    "    test_rmse = evaluate_rmse(params, test_set_x1, test_set_x2, test_set_y)\n",
    "    print(\"Test RMSE minibatch_gradient \" + str(batch_size) + \" : \" + str(test_rmse))\n",
    "\n",
    "    plt.plot(list(range(0, epochs)), train_error_list, c = 'b')\n",
    "    plt.plot(list(range(0, epochs)), validation_error_list, c = 'r')\n",
    "    plt.savefig(\"minibatch_gradient_descent.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_batch(epochs, learning_rate, alpha, batch_size):\n",
    "    num_params = 5\n",
    "    params = [0,0,0,0,0]\n",
    "    validation_error_list = []\n",
    "    train_error_list = []\n",
    "\n",
    "    tuple_list = []\n",
    "\n",
    "    for i in range(0, len(train_set_x1)):\n",
    "        tuple_list.append((train_set_x1[i], train_set_x2[i], train_set_y[i]))\n",
    "\n",
    "    random.shuffle(tuple_list)\n",
    "    train_data_list = tuple_list[:batch_size]\n",
    "\n",
    "    for i in range(0, epochs):\n",
    "        for j in range(0, len(train_data_list)):\n",
    "            for k in range(0, num_params):\n",
    "                # update = X^t(X*theta - y) + alpha * theta\n",
    "                theta = 0 if k == 0 else params[k]\n",
    "                update = (polynomial(params, train_data_list[j][0], train_data_list[j][1]) - train_data_list[j][2]) * polynomial_derivative(params,train_data_list[j][0], train_data_list[j][1], k) + alpha*theta\n",
    "                params[k] -= learning_rate*update\n",
    "\n",
    "        validation_error = evaluate_rmse(params, validation_set_x1, validation_set_x2, validation_set_y)\n",
    "        train_error = evaluate_rmse(params, train_set_x1, train_set_x2, train_set_y)\n",
    "        validation_error_list.append(validation_error)\n",
    "        train_error_list.append(train_error)\n",
    "\n",
    "        # print(\"Epoch: \" + str(i) + \"validation_error : \" + str(validation_error) + \"train_error : \" + str(train_error))\n",
    "    \n",
    "    test_rmse = evaluate_rmse(params, test_set_x1, test_set_x2, test_set_y)\n",
    "    print(\"Test RMSE ridge_regression \" + str(batch_size) + \" : \" + str(test_rmse))\n",
    "\n",
    "    plt.plot(list(range(0, epochs)), train_error_list, c = 'b')\n",
    "    plt.plot(list(range(0, epochs)), validation_error_list, c = 'r')\n",
    "    plt.savefig(\"minibatch_ridge_regression.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression_batch(epochs, learning_rate, alpha, batch_size):\n",
    "    num_params = 5\n",
    "    params = [0,0,0,0,0]\n",
    "    validation_error_list = []\n",
    "    train_error_list = []\n",
    "\n",
    "    tuple_list = []\n",
    "\n",
    "    for i in range(0, len(train_set_x1)):\n",
    "        tuple_list.append((train_set_x1[i], train_set_x2[i], train_set_y[i]))\n",
    "\n",
    "    random.shuffle(tuple_list)\n",
    "    train_data_list = tuple_list[:batch_size]\n",
    "\n",
    "    for i in range(0, epochs):\n",
    "        for j in range(0, len(train_data_list)):\n",
    "            for k in range(0, num_params):\n",
    "\n",
    "                theta = 0\n",
    "                if k > 0:\n",
    "                    if params[k] > 0:\n",
    "                        theta = 1\n",
    "                    elif params[k] < 0:\n",
    "                        theta = -1\n",
    "                    else:\n",
    "                        theta = 0\n",
    "\n",
    "                theta = 0 if k == 0 else params[k]\n",
    "                update = (polynomial(params, train_data_list[j][0], train_data_list[j][1]) - train_data_list[j][2]) * polynomial_derivative(params,train_data_list[j][0], train_data_list[j][1], k) + alpha*theta\n",
    "                params[k] -= learning_rate*update\n",
    "\n",
    "        validation_error = evaluate_rmse(params, validation_set_x1, validation_set_x2, validation_set_y)\n",
    "        train_error = evaluate_rmse(params, train_set_x1, train_set_x2, train_set_y)\n",
    "        validation_error_list.append(validation_error)\n",
    "        train_error_list.append(train_error)\n",
    "\n",
    "        # print(\"Epoch: \" + str(i) + \"validation_error : \" + str(validation_error) + \"train_error : \" + str(train_error))\n",
    "    \n",
    "    test_rmse = evaluate_rmse(params, test_set_x1, test_set_x2, test_set_y)\n",
    "    print(\"Test RMSE lasso_regression_minibatch with batch size \" + str(batch_size) + \" : \" + str(test_rmse))\n",
    "\n",
    "    plt.plot(list(range(0, epochs)), train_error_list, c = 'b')\n",
    "    plt.plot(list(range(0, epochs)), validation_error_list, c = 'r')\n",
    "    plt.savefig(\"minibatch_lasso_regression.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression(70, 0.001)    \n",
    "ridge_regression(70, 0.001, 0.01)\n",
    "lasso_regression(70, 0.001, 0.01)\n",
    "linear_regression_batch(70, 0.001, batch_size)\n",
    "ridge_regression_batch(70, 0.001, 0.01, batch_size)\n",
    "lasso_regression_batch(70, 0.001, 0.01, batch_size)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
